In this chapter we will cover some of theory behind symbolic execution. We will start by describing what it means to \emph{symbolically execute} a program and how we deal with potential branching. We will also describe the connection between a symbolic execution of a program, and a concrete one. 

\section{Principles of symbolic execution }
	
	In \cite{king76}, symbolic execution is described as a practical approach between simple program testing and program proving. At one extreme, program testing allows the programmer to get some level of confidence in the program, by running it on a well-selected sample of input values, but this sample size will be but a fraction of the possible input values. At the other extreme, program proving will give complete confidence in the programs correctness. To achieve this, one must provide a precise specification of correct behavior, as well as be able to perform perform formal proof steps to conclude that the program satisfies this specification. This is a challenging task, even for relatively simple programs. \emph{Symbolic execution} will serve as a practical middle ground between these two, in which we try to extend simple program testing to cover more general classes of inputs.  

\subsection{symbolically executing a program}
	
	\cite{CadarSen13} describes symbolic execution as follows:
	
	Any input to the program will be replaced with \emph{symbolic} values instead of concrete ones. Any operations on the symbolic values, will result in expressions over these, so our program state can be describes as a map from variable names to expressions over the input values. This also means that any return value from the execution will be such an expression. In \cite{king76} an example is given of a program written in a simple language with only signed integer values and arithmetic operations on these. In a symbolic context, this will translate to a program that operates on polynomials with integer coefficients.   
	
	To illustrate this, we consider the following simple program, that takes parameters $a, b, c$ and computes the sum:
	
	\begin{lstlisting}
	Fun sum(a, b, c) 
		var x = a + b
		var y = b + c
		var z = x + y - b
		return z
	\end{lstlisting} 
	
	If we run this program on concrete inputs, say $a = 2, b = 3, c = 4$, we would get the following execution:
	
	\begin{enumerate} 
		\item x = 2 + 3 $\Rightarrow$ y = 5
		\item y = 3 + 4 $\Rightarrow$ y = 7
		\item z = 5 + 7 - 3 $\Rightarrow$ z = 9
		\item return 9
	\end{enumerate}
	
	So we see that on the specific input $a = 2, b = 3, c = 4$, the program returns the correct value.
	\\
	Let us now run the program with symbols. Say that we the input the following symbolic values: $a = \alpha, b = \beta, c = \gamma$.
	The execution would then look like:
	
	\begin{enumerate}
		\item x = $\alpha + \beta$
		\item y = $\beta  + \gamma$
		\item z = $(\alpha + \beta) + (\beta + \gamma) - \beta$
		\item return $\alpha + \beta + \gamma$
	\end{enumerate}
	
	From this execution we can actually conclude that the program will return the correct result for any three integers that we give as input. 
	
	\subsection{Handling branching in a program}
		In the previous section we gave an example of a symbolic execution of a simple program that computes the sum of three integers. The program is an extremely simple case, in which the program will behave exactly the same for any possible input values. In reality however, most program languages have some way of allowing branching to happen, and with this feature, we cannot guarantee that the program will behave exactly the same for all inputs. If our program contains an expression like \emph{If $x > 2$ Then ... Else ... }, the program will behave differently depending on whether or nor $x > 2$. To encapsulate this, we introduce a \emph{path-constraint (PC)}, which is a boolean expression that will contain all properties that the input must satisfy to follow the given path. At the beginning of the execution, the $PC$ will be initialized with the value $true$ as no assumptions have been made yet (If we introduce pre-conditions on the input, the $PC$ will of course contain these). 
		At any \emph{if-expression} with condition $q$, during the execution we will look at the following to expressions
		
		\begin{enumerate}
			\item $ PC \land q$
			\item $ PC \land \neg q$.
		\end{enumerate}
		
		This gives a number of possible scenarios:
		
		\begin{itemize}
			\item \textbf{Only the first expression is satisfiable}: We will update $PC$ with $q$ so $PC \gets PC \land q$. Execution will continue by following the \emph{then}-branch.
			\item \textbf{If only the second expression is satisfiable}: We will update $PC$ with $\neg q$ so $PC \gets PC \land \neg q$. Execution will continue by following the \emph{else}-branch.
			\item \textbf{Both expressions are satisfiable}: In this case, the execution can follow both branches, so we \emph{fork} the program, by updating $PC$ with $PC \gets q$, and we make a copy of $PC$ and update this with $PC' gets \neg q$. This gives us two executions, one that follows the \emph{then}-branch with $PC$ and one that follows the \emph{else}-branch with $PC'$.  
		\end{itemize}
		
		We illustrate this with the following example:
		
	\begin{lstlisting}
	Fun pow(a, b) 
		var r = 1
		var i = 0
		while (i < b) 
			r = r*a
			i = i + 1 
		return r
		
		\end{lstlisting}
		
		If we assign $a = \alpha$ and $b = \beta$, we get the following execution:
		
		\begin{itemize}
			\item PC is initialized to $true$
			\item $r \gets 1$
			\item $i \gets 0$
			\item We hit a branching point, so we check if $true \land (0 < \beta)$ and $true \land \neg (0 < \beta)$ are satisfiable. Since they both are, we must fork:
			\item \textbf{Case} $ \neg (0 < \beta) $: $PC' \gets true \land \neg (0 < \beta) $.
				The program returns $1$. So we can conclude that the program returns 1 when $\beta \leq 0$.
			\item \textbf{Case} $ (0 < \beta)$: $PC \gets true \land (0 < \beta)$.
			\subitem  $ r \gets 1 \cdot a$
			\subitem  $ i \gets 0 + 1$
			\item We hit a branching point again, so we check if $true \land (0 < \beta) \land (1 < \beta)$ and $true \land (0 < \beta) \land \neg (1 < \beta)$ are satisfiable. Since both they are, we fork again:
			\item \textbf{Case} $ \neg(1 < \beta)$: $PC' \gets true \land (0 < \beta) \land \neg (1 < \beta)$. The program returns $\alpha$. So we can conclude that the program returns $\alpha$ when $ \beta = 1$.
			\item \textbf{Case} $ 1 < \beta$: $PC \gets true \land (0 < \beta) \land (1 < \beta)$.
			\subitem $r \gets a*a$
			\subitem $i \gets 1 + 1$
			\item We hit a branching point $\ldots$	
		\end{itemize}
		
		An important property of the \emph{path-constraint} is that it can never become identically false. To see why this is the case, we have to look at the possible updates of $PC$. At the start of an execution, it will be initialized with the value $true$. At any branching point, it will be updated with exactly one of the expressions $PC \land q$ and $PC \land \neg q$, and only if the given expression is satisfiable. So $PC$ will never end up looking like $\ldots \land q \land \ldots \land \neg q \land \ldots $ for some condition $q$. What this means is that when the program terminates at the end of some execution path, $PC$ will be a satisfiable formula over the symbolic values, which means that we can solve the constraints and derive a set of concrete values which will follow the exact same path if we execute the program normally.   
		
		%TODO Find better titles for sections 
		\subsection{Limitations of symbolic execution}
		
		\subsubsection{Infinite execution trees}
			As demonstrated in the last example in the previous sections, a symbolic execution can easily become infinite as soon as we introduce branching and some looping structure. This is further illustrated if we consider an execution tree for a program. In \cite{king76}, they are described by enumerating each statement, and let each node in the tree, correspond to an execution of one of the command. The edges going out from a node corresponds to the transition from one statement to the next. From this we can see that non forking statements will only have a single edge outgoing, while forking statements will have two. The forking edges will then correspond to splitting up the execution and following a different path, with a different \emph{path-constraint}. As an example, we can look at the execution tree for the program \emph{pow}, that computes $a^b$. 
			
			\Tree[.1 [.2 [.3 [.4 [.5 [.6 [.3 [.4 [.5 [.6 [.3 [.$\vdots$ ] [.{7: return $\alpha^2$ when $\beta = 2$  }
			] ] ]  ] ] [.{7: return $\alpha$ when $\beta = 1$} ]  ]  ] ] ] [.{7: return 1 when $\beta <= 0$ } ] ]  ]  ] 
							
		
			As we can see, this tree is infinite, since $b = \beta$ and $\beta$ can be arbitrarily large. In this case, our symbolic execution would run forever if we insisted on exhaustively exploring all possible paths. This illustrates one of the limitations of symbolic execution. For programs with infinite execution trees, we simply cannot exhaust all possible inputs, so we have to restrict our testing to exploring only a finite number of paths. 
			
			(Maybe write something about induction over trees, and finite trees allowing for exhaustive search)
			
			\subsubsection{the ability(or inability) to decide whether a given path is feasible}
			
			(Write something about restrictions on SMT-solvers e.g SAT being NP Complete and some theories may be undecidable)