In this chapter we will cover some of theory behind symbolic execution. We will start by describing what it means to \emph{symbolically execute} a program and how we deal with potential branching. We will also describe the connection between a symbolic execution of a program, and a concrete one. Furthermore we will relate this technique to alternatives such as formally proving correctness. 

\section{Principles of symbolic execution }
	in \cite{king76}, the author looks at a spectrum of ensuring that a program functions as expected. In one extreme we have program testing, where the programmer specifies a sample of input. The program is then run on this sample, and if the program runs as intended, the programmer can be sure that for this sample the program is correct. choosing a proper sample of inputs will then enable the programmer to have some level of confidence that the program runs correctly. But since most programs can take virtually an infinite number of different inputs, one can never be absolutely sure that the program is free of bugs. At the other end of the spectrum is formal program verification. 
	This requires a proper mathematical specification of the program, and a proof procedure that will verify that this specification is correct w.r.t some formal requirements. 
	This will give the programmer a complete confidence that the program is in fact correct. This comes at the cost of producing a proper and adequately strong specification, performing sound steps in the proof procedure as well as having a strong set of requirements. 
	We find symbolic execution somewhere in the middle of this spectrum. While we do not build a formal specification and apply any sort of program verification, we do not restrict our self to specific samples of input. Instead, we try to test the program for whole classes of inputs (such as all integers that are a power of two), which is represented by symbolic values instead of actual ones.

\subsection{symbolically executing a program}
	
	Symbolic execution is described in \cite{king76} by imagining a simple programming language which restricts all variables to signed integers. The language supports the usual arithmetic operations on these integers, as well as a conditional expression to decide whether a given value is $ \leq 0$. To execute such a program symbolically, we allow the variables to also take \emph{symbolic} values, where these symbols represent some signed integer. We also allow arithmetic operations on these symbols, and for variables to hold such expressions. This means variables in fact contains polynomials over the integers.
	To illustrate this, we consider the following simple program, that takes parameters $a, b, c$ and computes the sum:
	
	\begin{verbatim}
		Fun sum(a, b, c) {
			var x = a + b
			var y = b + c
			var z = x + y - b
			return z
		}
	\end{verbatim} 
	
	If we run this program on concrete inputs, say $a = 2, b = 3, c = 4$, we would get the following execution:
	
	\begin{enumerate} 
		\item x = 2 + 3 $\Rightarrow$ y = 5
		\item y = 3 + 4 $\Rightarrow$ y = 7
		\item z = 5 + 7 - 3 $\Rightarrow$ z = 9
		\item return 9
	\end{enumerate}
	
	So we see that on the specific input $a = 2, b = 3, c = 4$, the program returns the correct value.
	\\
	Let us now run the program with symbols. Say that we the input the following symbolic values: $a = \alpha, b = \beta, c = \gamma$.
	The execution would then look like:
	
	\begin{enumerate}
		\item x = $\alpha + \beta$
		\item y = $\beta  + \gamma$
		\item z = $(\alpha + \beta) + (\beta + \gamma) - \beta$
		\item return $\alpha + \beta + \gamma$
	\end{enumerate}
	
	From this execution we can actually conclude that the program will return the correct result for any three integers that we give as input. 
	
	\subsection{Handling branching in a program}
		In the previous section we gave an example of a symbolic execution of a simple program that computes the sum of three integers. The program is an extremely simple case, in which the program will behave exactly the same for any possible input values. In reality however, most program languages have some way of allowing branching to happen, and with this feature, we cannot guarantee that the program will behave exactly the same for all inputs. If our program contains an expression like \emph{If $x > 2$ Then ... Else ... }, the program will behave differently depending on whether or nor $x > 2$. To encapsulate this, we introduce a \emph{path-constraint (PC)}, which is a boolean expression that will contain all properties that the input must satisfy to follow the given path. At the beginning of the execution, the $PC$ will be initialized with the value $true$ as no assumptions have been made yet (If we introduce pre-conditions on the input, the $PC$ will of course contain these). 
		At any \emph{if-expression} with condition $q$, during the execution we will look at the following to expressions
		
		\begin{enumerate}
			\item $ PC \supset q$
			\item $ PC \supset \neg q$
		\end{enumerate}
		
		where expression 1 tells us that $q$ is contained in the $PC$, and expression 2 tells us that the opposite of $q$ is contained in the $PC$. It is clear that at most one of these expressions can be true at any given time, which leaves us with three possible outcomes. If expression 1 holds, we simply follow the then-branch, and if expression 2 holds, we follow the else branch. In both these outcomes, $PC$ is not updated since it already contains the property that leads to the given branch that was chosen. The third outcome happens when neither expression holds. At this point we cannot simply follow one of the branches, so we must fork our execution into two parallel executions, one where we assume that $q$ holds and one where we assume $\neg q$ holds. This will produce two new $PCs$, namely 
		
		\begin{align*}
			PC' & \gets PC \land q \\
			PC''  & \gets PC \land \neg q
		\end{align*}
		
		and from here we will have two seperate executions, one with $PC'$ that will follow the then-branch, and one with $PC''$ that will follow the else-branch. 
		It is important to note that the $PC$ can never become identically $false$. To see this we first note that $PC$ will always start with the value $true$, and all other updates will be of the form $PC \gets PC \land r$ where 
		$r \in \{q, \neg q\}$. But these updates only happen whenever we cannot infer $q$ or $\neg q$ from the existing value of $PC$ and it will only receive exactly of $q$ or $\neg q$. So it is never possible to have a state of $PC$ in which we have $ \ldots \land q \land \ldots \land \neg q \ldots$. 
		
		As in \cite{king76}, we can illustrate this with the following program that computes $a^b$.
		
		\begin{verbatim}
			Fun pow(a, b) {
			var r = 1
			var i = 0
			while (i < b) {
				r = r*a
				i = i + 1 
			}
			return r
		\end{verbatim}
		
		If we assign $a = \alpha$ and $b = \beta$, we get the following execution:
		
		\begin{itemize}
			\item PC is initialized to $true$
			\item $r \gets 1$
			\item $i \gets 0$
			\item We hit a branching point, so we check if $true \supset (0 < \beta)$ or $true \supset \neg (0 < \beta)$. Since neither of these hold, we must fork:
			\item \textbf{Case} $ \neg (0 < \beta) $: $PC \gets true \land \neg (0 < \beta) $.
				The program returns $1$. So we can conclude that the program returns 1 when $\beta \leq 0$.
			\item \textbf{Case} $ (0 < \beta)$: $PC \gets true \land (0 < \beta)$.
			\subitem  $ r \gets 1 \cdot a$
			\subitem  $ i \gets 0 + 1$
			\item We hit a branching point again, so we check if $true \land (0 < \beta) \supset 1 < \beta$ or $true \land (0 < \beta) \supset \neg (1 < \beta)$. Since neither of these holds, we fork again:
			\item \textbf{Case} $ \neg(1 < \beta)$: $PC \gets true \land (0 < \beta) \land \neg (1 < \beta)$. The program returns $\alpha$. So we can conclude that the program returns $\alpha$ when $ \beta = 1$.
			\item \textbf{Case} $ 1 < \beta$: $PC \gets true \land (0 < \beta) \land (1 < \beta)$.
			\subitem $r \gets a*a$
			\subitem $i \gets 1 + 1$
			\item We hit a branching point $\ldots$	
		\end{itemize}
		
	\subsection{Symbolic execution trees}